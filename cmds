
rolling update demo - image: kodekloud/webapp-color:v2     v1(blue) v2(red) v3(green) strategy- recreate or rollingupdate 25% doen 25% up

#kubectl run nginx --image=nginx
pod/nginx created

kuebctl get pods

kubectl describe pod newpods-6rn6n

kubectl get pods -o wide

kubectl delete pod <podname>

kubectl run redis --image=redis123 --dry-run=client -o yaml > redis-definition.yaml

____
  cat redis-definition.yaml 
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: redis
  name: redis
spec:
  containers:
  - image: redis123
    name: redis
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}
_____

kubectl create -f redis-definition.yaml
pod/redis created

kubectl edit pod redis
chnage--name from redis123 to redis

kubectl get replicaset

kubectl get replicaset
NAME              DESIRED   CURRENT   READY   AGE
new-replica-set   4         4         0       20s

kubectl get pods

kubectl delete pod new-replica-set-qkdd9 new-replica-set-f6bhg new-replica-set-m454x new-replica-set-zs99r

cat replicaset-definition-1.yaml 
__
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: replicaset-1
spec:
  replicas: 2
  selector:
    matchLabels:
      tier: frontend
  template:
    metadata:
      labels:
        tier: frontend
    spec:
      containers:
      - name: nginx
        image: nginx

____


kubectl delete -f  replicaset-definition-1.yaml

kubectl scale rs new-replica-set --replicas=2 

kubectl get service

kubectl describe service

kubectl get deployments

cat service-definition-1.yaml 
---
apiVersion: v1
kind: Service
metadata:
  name: webapp-service 
  namespace: default
spec:
  ports:
  - nodePort: 30080 
    port: 8080
    targetPort: 8080 
  selector:
    name: simple-webapp
  type: NodePort

_____
kubectl get ns

kubectl get pods -n research

kubectl run redis --image=redis -n finance

kubectl get pods -A | grep blue

kubectl run nginx-pod --image=nginx:alpine

kubectl run redis --image=redis:alpine --dry-run=client -oyaml > redis-pod.yaml   0r kubectl run redis -l tier=db --image=redis:alpine

kubectl apply -f redis-pod.yaml

kubectl expose pod redis --port=6379 --name redis-service

kubectl create deployment  webapp --image=kodekloud/webapp-color --replicas=3

kubectl run custom-nginx --image=nginx --port=8080

kubectl create namespace dev-ns

kubectl create deployment redis-deploy --image=redis --replicas=2 -n dev-ns

kubectl run httpd --image=httpd:alpine --port=80 --expose

kubectl get pods --selector env=dev --no-headers | wc -l

kubectl describe node node01 | grep -i taints

kubectl taint nodes node01 spray=mortein:NoSchedule
cat sample.yaml 
---
apiVersion: v1
kind: Pod
metadata:
  name: mosquito
spec:
  containers:
  - image: nginx
    name: mosquito


Create another pod named bee with the nginx image, which has a toleration set to the taint mortein
---
apiVersion: v1
kind: Pod
metadata:
  name: bee
spec:
  containers:
  - image: nginx
    name: bee
  tolerations:
  - key: spray
    value: mortein
    effect: NoSchedule
    operator: Equal
___
kubectl taint nodes controlplane node-role.kubernetes.io/control-plane:NoSchedule-
node/controlplane untainted

kubectl label node node01 color=blue

kubectl create deployment blue --image=nginx --replicas=3

___
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: blue
spec:
  replicas: 3
  selector:
    matchLabels:
      run: nginx
  template:
    metadata:
      labels:
        run: nginx
    spec:
      containers:
      - image: nginx
        imagePullPolicy: Always
        name: nginx
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: color
                operator: In
                values:
                - blue

____

---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  labels:
    app: elasticsearch
  name: elasticsearch
  namespace: kube-system
spec:
  selector:
    matchLabels:
      app: elasticsearch
  template:
    metadata:
      labels:
        app: elasticsearch
    spec:
      containers:
      - image: registry.k8s.io/fluentd-elasticsearch:1.20
        name: fluentd-elasticsearch


______

static pods

kubectl run --restart=Never --image=busybox static-busybox --dry-run=client -o yaml --command -- sleep 1000 > /etc/kubernetes/manifests/static-busybox.yaml



____
##metrics##
kubectl top node --sort-by='cpu' --no-headers | head -1

kubectl top node --sort-by='memory' --no-headers | h

kubectl top pod --sort-by='memory' --no-headers | head -1

kubectl proxy

kubectl port-forward

git clone https://github.com/kodekloudhub/kubernetes-metrics-server.git


dashboard

https://redlock.io/blog/cryptojacking-tesla

https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/

https://github.com/kubernetes/dashboard

https://www.youtube.com/watch?v=od8TnIvuADg 

https://blog.heptio.com/on-securing-the-kubernetes-dashboard-16b09b1b7aca

https://github.com/kubernetes/dashboard/blob/master/docs/user/access-control/creating-sample-user.md


releases
https://github.com/kubernetes/kubernetes/releases

https://github.com/kubernetes/design-proposals-archive/blob/main/release/versioning.md

https://github.com/kubernetes/design-proposals-archive/blob/main/api-machinery/api-group.md

https://blog.risingstack.com/the-history-of-kubernetes/

https://kubernetes.io/docs/setup/version-skew-policy

______________ side car container_____________

---
apiVersion: v1
kind: Pod
metadata:
  name: app
  namespace: elastic-stack
  labels:
    name: app
spec:
  containers:
  - name: app
    image: kodekloud/event-simulator
    volumeMounts:
    - mountPath: /log
      name: log-volume

  - name: sidecar
    image: kodekloud/filebeat-configured
    volumeMounts:
    - mountPath: /var/log/event-simulator/
      name: log-volume

  volumes:
  - name: log-volume
    hostPath:
      # directory location on host
      path: /var/log/webapp
      # this field is optional
      type: DirectoryOrCreate





######install kube sec for scanning images#####

wget https://github.com/controlplaneio/kubesec/releases/download/v2.13.0/kubesec_linux_amd64.tar.gz

tar -xvf  kubesec_linux_amd64.tar.gz

mv kubesec /usr/bin/
#####

###kubesec scan /root/node.yaml  > /root/kubesec_report.json^C

## cat /root/node.yaml 
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: "2024-09-21T11:41:42Z"
  name: node
  namespace: default
  resourceVersion: "988"
  uid: 71dad08e-244c-4181-9fbb-5e3b3ca2386d
spec:
  containers:
  - image: gcr.io/google-samples/node-hello:1.0
    imagePullPolicy: IfNotPresent
    name: node
    resources: {}
    securityContext:
      privileged: true
      readOnlyRootFilesystem: false
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
    volumeMounts:
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: kube-api-access-vdh7g
      readOnly: true
  dnsPolicy: ClusterFirst
  enableServiceLinks: true
  nodeName: controlplane
  preemptionPolicy: PreemptLowerPriority
  priority: 0
  restartPolicy: Always
  schedulerName: default-scheduler
  securityContext: {}
  serviceAccount: default
  serviceAccountName: default
  terminationGracePeriodSeconds: 30
  tolerations:
  - effect: NoExecute
    key: node.kubernetes.io/not-ready
    operator: Exists
    tolerationSeconds: 300
  - effect: NoExecute
    key: node.kubernetes.io/unreachable
    operator: Exists
    tolerationSeconds: 300
  volumes:
  - name: kube-api-access-vdh7g
    projected:
      defaultMode: 420
      sources:
      - serviceAccountToken:
          expirationSeconds: 3607
          path: token
      - configMap:
          items:
          - key: ca.crt
            path: ca.crt
          name: kube-root-ca.crt
      - downwardAPI:
          items:
          - fieldRef:
              apiVersion: v1
              fieldPath: metadata.namespace
            path: namespace
status:
  conditions:
  - lastProbeTime: null
    lastTransitionTime: "2024-09-21T11:41:42Z"
    status: "False"
    type: PodReadyToStartContainers
  - lastProbeTime: null
    lastTransitionTime: "2024-09-21T11:41:42Z"
    status: "True"
    type: Initialized
  - lastProbeTime: null
    lastTransitionTime: "2024-09-21T11:41:42Z"
    message: 'containers with unready status: [node]'
    reason: ContainersNotReady
    status: "False"
    type: Ready
  - lastProbeTime: null
    lastTransitionTime: "2024-09-21T11:41:42Z"
    message: 'containers with unready status: [node]'
    reason: ContainersNotReady
    status: "False"
    type: ContainersReady
  - lastProbeTime: null
    lastTransitionTime: "2024-09-21T11:41:42Z"
    status: "True"
    type: PodScheduled
  containerStatuses:
  - image: gcr.io/google-samples/node-hello:1.0
    imageID: ""
    lastState: {}
    name: node
    ready: false
    restartCount: 0
    started: false
    state:
      waiting:
        reason: ContainerCreating
  hostIP: 192.23.60.3
  hostIPs:
  - ip: 192.23.60.3
  phase: Pending
  qosClass: BestEffort
  startTime: "2024-09-21T11:41:42Z"



###  cat /root/kubesec_report.json 
[
  {
    "object": "Pod/node.default",
    "valid": true,
    "fileName": "/root/node.yaml",
    "message": "Failed with a score of -27 points",
    "score": -27,
    "scoring": {
      "critical": [
        {
          "id": "Privileged",
          "selector": "containers[] .securityContext .privileged == true",
          "reason": "Privileged containers can allow almost completely unrestricted host access",
          "points": -30
        }
      ],
      "passed": [
        {
          "id": "ServiceAccountName",
          "selector": ".spec .serviceAccountName",
          "reason": "Service accounts restrict Kubernetes API access and should be configured with least privilege",
          "points": 3
        }
      ],
      "advise": [
        {
          "id": "ApparmorAny",
          "selector": ".metadata .annotations .\"container.apparmor.security.beta.kubernetes.io/nginx\"",
          "reason": "Well defined AppArmor policies may provide greater protection from unknown threats. WARNING: NOT PRODUCTION READY",
          "points": 3
        },
        {
          "id": "SeccompAny",
          "selector": ".metadata .annotations .\"container.seccomp.security.alpha.kubernetes.io/pod\"",
          "reason": "Seccomp profiles set minimum privilege and secure against unknown threats",
          "points": 1
        },
        {
          "id": "LimitsCPU",
          "selector": "containers[] .resources .limits .cpu",
          "reason": "Enforcing CPU limits prevents DOS via resource exhaustion",
          "points": 1
        },
        {
          "id": "LimitsMemory",
          "selector": "containers[] .resources .limits .memory",
          "reason": "Enforcing memory limits prevents DOS via resource exhaustion",
          "points": 1
        },
        {
          "id": "RequestsCPU",
          "selector": "containers[] .resources .requests .cpu",
          "reason": "Enforcing CPU requests aids a fair balancing of resources across the cluster",
          "points": 1
        },
        {
          "id": "RequestsMemory",
          "selector": "containers[] .resources .requests .memory",
          "reason": "Enforcing memory requests aids a fair balancing of resources across the cluster",
          "points": 1
        },
        {
          "id": "CapDropAny",
          "selector": "containers[] .securityContext .capabilities .drop",
          "reason": "Reducing kernel capabilities available to a container limits its attack surface",
          "points": 1
        },
        {
          "id": "CapDropAll",
          "selector": "containers[] .securityContext .capabilities .drop | index(\"ALL\")",
          "reason": "Drop all capabilities and add only those required to reduce syscall attack surface",
          "points": 1
        },
        {
          "id": "ReadOnlyRootFilesystem",
          "selector": "containers[] .securityContext .readOnlyRootFilesystem == true",
          "reason": "An immutable root filesystem can prevent malicious binaries being added to PATH and increase attack cost",
          "points": 1
        },
        {
          "id": "RunAsNonRoot",
          "selector": "containers[] .securityContext .runAsNonRoot == true",
          "reason": "Force the running image to run as a non-root user to ensure least privilege",
          "points": 1
        },
        {
          "id": "RunAsUser",
          "selector": "containers[] .securityContext .runAsUser -gt 10000",
          "reason": "Run as a high-UID user to avoid conflicts with the host's user table",
          "points": 1
        }
      ]
    }
  }
]

###In node.yaml template change privileged: true to privileged: false under securityContext: need to updated to get scan PASS###


##################trivy is tool used to scan docker images on ubntu instance ############################
#Add the trivy-repo

#apt-get  update
apt-get install wget apt-transport-https gnupg lsb-release -y
wget -qO - https://aquasecurity.github.io/trivy-repo/deb/public.key | gpg --dearmor | sudo tee /usr/share/keyrings/trivy.gpg > /dev/null
echo "deb [signed-by=/usr/share/keyrings/trivy.gpg] https://aquasecurity.github.io/trivy-repo/deb $(lsb_release -sc) main" | sudo tee -a /etc/apt/sources.list.d/trivy.list

#Update Repo and Install trivy
apt-get update
apt-get install trivy

#trivy image  39286ab8a5e1 | head
2024-09-21T08:05:01-04:00       INFO    [vuln] Vulnerability scanning is enabled
2024-09-21T08:05:01-04:00       INFO    [secret] Secret scanning is enabled
2024-09-21T08:05:01-04:00       INFO    [secret] If your scanning is slow, please try '--scanners vuln' to disable secret scanning
2024-09-21T08:05:01-04:00       INFO    [secret] Please see also https://aquasecurity.github.io/trivy/v0.55/docs/scanner/secret#recommendation for faster secret detection
2024-09-21T08:05:09-04:00       INFO    Detected OS     family="debian" version="12.7"
2024-09-21T08:05:09-04:00       INFO    [debian] Detecting vulnerabilities...   os_version="12" pkg_num=149
2024-09-21T08:05:09-04:00       INFO    Number of language-specific files       num=0
2024-09-21T08:05:09-04:00       WARN    Using severities from other vendors for some vulnerabilities. Read https://aquasecurity.github.io/trivy/v0.55/docs/scanner/vulnerability#severity-selection for details.

39286ab8a5e1 (debian 12.7)
==========================
Total: 145 (UNKNOWN: 0, LOW: 87, MEDIUM: 40, HIGH: 13, CRITICAL: 5)

┌────────────────────┬─────────────────────┬──────────┬──────────────┬─────────────────────────┬─────────────────┬──────────────────────────────────────────────────────────────┐
│      Library       │    Vulnerability    │ Severity │    Status    │    Installed Version    │  Fixed Version  │                            Title                             │
├────────────────────┼─────────────────────┼──────────┼──────────────┼─────────────────────────┼─────────────────┼──────────────────────────────────────────────────────────────┤
│ apt                │ CVE-2011-3374       │ LOW      │ affected     │ 2.6.1                   │                 │ It was found that apt-key in apt, all versions, do not       │
│                    │                     │          │              │                         │                 │ correctly...  



#crictl pull public.ecr.aws/docker/library/python:3.12.4

## Scan the vulnerability
trivy image --output /root/python_12.txt public.ecr.aws/docker/library/python:3.12.4

##head /root/python_12.txt public.ecr.aws/docker/library/python:3.12.4
==> /root/python_12.txt <==

public.ecr.aws/docker/library/python:3.12.4 (debian 12.6)
=========================================================
Total: 1344 (UNKNOWN: 3, LOW: 538, MEDIUM: 656, HIGH: 132, CRITICAL: 15)

┌──────────────────────────────┬─────────────────────┬──────────┬──────────────┬──────────────────────────────┬────────────────────┬──────────────────────────────────────────────────────────────┐
│           Library            │    Vulnerability    │ Severity │    Status    │      Installed Version       │   Fixed Version    │                            Title                             │
├──────────────────────────────┼─────────────────────┼──────────┼──────────────┼──────────────────────────────┼────────────────────┼──────────────────────────────────────────────────────────────┤
│ apt                          │ CVE-2011-3374       │ LOW      │ affected     │ 2.6.1                        │                    │ It was found that apt-key in apt, all versions, do not       │
│                              │                     │          │              │                              │                    │ correctly...                 




#trivy image --severity HIGH --output /root/python.txt public.ecr.aws/docker/library/python:3.9-bullseye

#trivy image --input alpine.tar --format json --output /root/alpine.json










